---
title       : Diminishing propensity score calipers
author      : Ben B. Hansen (bbh@umich.edu)
date        : UM Statistics, Nov. 2019
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      #
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
--- 

# Outline

1. <span class = 'red'>Problem: subject comparability in groupwise comparisons</span>
2. Propensity score caliper matching
3. Diminishing calipers
4. Applications & discussion    




--- &twocol    

## Comparability of subjects in groupwise comparisons: canonical scenario with 1-dimensional covariate

*** =left

> - The quasi-experimental setup: $X$, $Y$; $Z \in \{0,1\}$, $Y_c$.
> - <embed height="400px" width="400px" src="./images/pretest-comp-group-design.jpg">

*** =right

> - Observations well outside support of contrasting group should be excluded.
> - Wasteful to exclude members of $\{ i: Z_i=1\}$ because their $x$ is barely outside $\mathrm{range}(\{x_j: Z_j=0\})$.
> -  May distort research question, too.
> - We say $x$ is close to $w$ when we expect $E(Y_{c}|X=x) \approx E(Y_{c}|X=w)$.
> - (...or that $\text{Pr}(Z| X=x) \approx \text{Pr}(Z| X=w)$.)
> - When we lack confidence that $E(Y_{c}|X=x) \approx E(Y_{c}|X=w)$ (or $\text{Pr}(Z| X=x) \approx \text{Pr}(Z| X=w)$), then $x$ and $w$ are not close.   

---

##  Covariate balance -- a separate topic

> - Groupwise comparability is often discussed in terms of whether $\bar{x}_{z=1} \approx \bar{x}_{z=0}$, all or most $X$s.
> - The question presupposes at least rough comparability of subjects: otherwise, mean comparisons can be misleading.
<embed src="./images/balancing-stones.jpg">
> - A topic for another day...

---

## Statistical assumptions for general $X$es

>- Now, multiple $X$es: $\mathbf{X}$, not $X$.
>- Strong ignorability conditions:
>    - No unmeasured confounding: $Y_c \perp Z | \mathbf{X}$.
>    - Overlap: $\mathrm{Pr}(Z=1  | \mathbf{X}) < 1$.      
>- No unmeasured confounding says, there are natural experiments in each "cell" of $\mathbf{X}$.
>- Problem: if multiple ${X}$es, many observations will be all alone in their cells.
>- Rosenbaum & Rubin's (1983) solution is match on an estimate of the propensity score (PS), $\widehat{\mathrm{Pr}}(Z=1| \mathbf{X}=\mathbf{x})$. (Or $g\{\widehat{\mathrm{Pr}}(Z=1| \mathbf{x})\}$, e.g. $g\equiv$ logit.)
>- Second problem: In a cell $\mathbf{x}_0$ s.t. $\mathrm{Pr}(Z=1| \mathbf{X}=\mathbf{x}_0)=1$, there's no data to estimate $\mathrm{Pr}(Y_c \in \cdot |  \mathbf{X}=\mathbf{x}_0)$, even as $n \uparrow \infty$.  (Whether you match on the PS, weight for it, ...)
>- The second problem motivates the (clearly flawed) method of strict overlap, i.e. reducing analytic sample to region of common support on $\widehat{\mathrm{PS}}$: $$\cap_{z=0,1}[\min(\{\widehat{PS}_i:Z_i=z\}), \max(\{\widehat{PS}_i:Z_i=z\})] .$$ 


--- &twocol
##  Example 1: violence & public infrastructure in Medell&iacute;n


*** =left

- Medellin, Colombia (population 2 million)
- As of early 2000s, 60% poverty rate, 20% unemployment, homicide 185 per 100K
- High residential segregation, w/ concentrated poverty in surrounding hills.
- 2004-2006: gondolas connect some but not all to city center.
- Cerda et al (2012, _Am J. Epideomiol._) study effects on neighborhood violence, propensity matching treatment neighborhoods to control.

*** =right

<embed height="625px" width="500px" src="./images/medellin-conc-pov.jpg"> 

*** =pnotes

- In contrast, Detroit's  2007 murder rate was 47/100K, and East St. Louis's was 102 per 100K.

--- &twocolleftwider


## Propensity scores in the Medellin study

*** =left

>- Small matching problem: $n_1=25$, $n_0=23$ (neighborhoods).
>- \(|\bar{x}_{z=1} - \bar{x}_{z=0}|\)s not too large, but PS matching made them smaller.
>- Figure shows PS we matched on - the $\mathbf{x}\hat{\beta}$ from a logistic regression (Cerda et al 2012, _Am J Epi_).
>- Region of strict overlap contains only 6/25 $t$s and 4/23 $c$s!
>- Yet Cerda et al matched all 48 neighborhoods.  (Using "full matching", which permits many-to-one matches.)

*** =right

<embed height="625px" width="400px" src="./images/boxplot-Medellin-logit.jpg">


--- 
# Outline

1. Problem: subject comparability in groupwise comparisons
2. <span class = 'red'>Propensity score caliper matching</span>
3. Diminishing calipers
4. Applications & discussion    





--- &twocolleftwider


## Matching with PS calipers (Rosenbaum & Rubin) 

>- For matching, useful to specify a tolerance or _caliper_ (Althauser & Rubin, 1970). 
>- Absent unmeasured confounding, matches with small PS differences mimic paired random assignment (R.&R. 1983). 
>- In effect, PS caliper (R.&R. 1985) adjudicates "closeness" to region of common support. 

*** =left

>- R. & R. (1985), Rubin & Thomas (2000) recommend $s_p(\widehat{\mathrm{PS}})/4$.
>- (Not $\widehat{\mathrm{PS}}$ the estimated probability, $\widehat{\mathrm{PS}}$ the estimated index function.
>  I.e., logits of estimated probabilities.)
>- The other widely used method (e.g. Austin & Lee 2009, Lunt 2013), besides requiring strict overlap.

*** =right

<embed height="400px" width="200px" src="./images/boxplot-Medellin-logit.jpg">



--- &twocol

## Room for improvement (in caliper = $.25s_p$)

>- $s_p$ does not tend to 0. For large $n$, matches as crude as $.25s_p$ aren't very RCT-like.  (Even if there's no unmeasured confounding.) 
>- In the Medellin study (smallish $n$), $.25s_p(\widehat{\mathrm{PS}})$ caliper excludes 56% of treatment group.
>- At the same time, in that study it's almost tenable ($p=.04$) that all PSes are the same!

*** =left

>- <embed height="400px" width="400px" src="./images/boxplots-Medellin-logit.jpg">

*** =right

>- Moral: in small studies, $.25s_p(\widehat{\mathrm{PS}})$ may be too strict. 
>- In large studies, PS may be estimable w/ much more precision than $.25s_p(\widehat{\mathrm{PS}})$.  At same time, more potential controls. 
>- Moral: in large studies, $.25s_p(\widehat{\mathrm{PS}})$ may be too loose.

--- 

# Outline

1. Problem: subject comparability in groupwise comparisons
2. Propensity score caliper matching
3. <span class = 'red'>Diminishing calipers</span>
4. Applications & discussion    


---

## Sampling variability of paired index differences, i

>- Intuition: Even if we had matched **perfectly** on index $\mathbf{x}\beta$, there would still be matched differences in $\mathbf{x}\hat\beta$.  Let's estimate the size of these differences & use result to define caliper.    
>    - Should imply that as $n\uparrow \infty$, caliper $\downarrow 0$. If also $\hat\beta - \beta \stackrel{P}{\rightarrow} 0$, expect $\max_{i \sim j} |(\mathbf{x}_i - \mathbf{x}_j)\beta|\downarrow 0$, where $i \sim j$ denotes that $i$ is matched to $j$.
>    - Excludes treatment subjects with larger-than-chance separation from controls on $\mathbf{x}\hat\beta$s.    
>- Assuming well-conditioned g.l.m., model dimension $p = o({n}/{\log (n)})$, He & Shao (2000) showed that $|\hat\beta - \beta|_2 = O_P[( {p}/{n})^{1/2}]$.  
>- The error of estimation of paired difference $(\mathbf{x}_i - \mathbf{x}_j)\beta$ is $(\mathbf{x}_i - \mathbf{x}_j)(\hat{\beta} - \beta)$.  Given $\nu$ pairs $(i, j)$, $i < j$ with $i \sim j$, the mean-square of paired errors is
\[
\frac{1}{\nu} \sum_{i \sim j; i < j}
[(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)]^2 =
(\hat\beta - \beta)' \left[\frac{1}{\nu}
                   \sum_{i \lesssim j} (\mathbf{x}_i - \mathbf{x}_j)'(\mathbf{x}_i - \mathbf{x}_j)\right] =: (\hat\beta - \beta)' (2 S^{(p)} )
(\hat\beta - \beta).
\]

---

## Sampling variability of paired index differences, ii

>- We wish to estimate the magnitude of $(\hat\beta - \beta)' (2 S^{(p)}
) (\hat\beta - \beta)$, with $S^{(p)}=$ covariance of $x$s w/in pairs $i \sim j$.
>- Assuming $|S^{(p)}|_2 = O_P(1)$, $(\hat\beta - \beta)' S^{(p)}
(\hat\beta - \beta) = O_P(p/n)$ (He & Shao, 2000).
>- Define $\tilde\beta = \beta + n^{-1}\sum_{i=1}^{n}\mathrm{IC}(Y_i, {Z}_{i}, \mathbf{x}_{i}; \beta)$. ("IC"=influence curve.) By other results of H. & S.,
\[
(\hat\beta - \beta)' S^{(p)}
(\hat\beta - \beta)  =
(\tilde\beta - \beta)' S^{(p)}
(\tilde\beta - \beta) + o_P( {p}/{n} ).
\]
>- $(\tilde\beta - \beta)' S^{(p)}
(\tilde\beta - \beta)$ is easier to estimate.  For logistic regression & certain other g.l.m.s, conventional estimates $\hat{C}_\hat\beta$ of $\text{Cov}(\hat\beta)$ satisfy $n|\hat{C}_\hat\beta - {C}_{\tilde\beta}|_2 \stackrel{P}{\rightarrow} 0$, ${C}_{\tilde\beta}:= \text{Cov}(\tilde\beta)$, under conditions similar to those H. & S. (2000) used for consistency of $\hat\beta$ (Hansen '19). 
>- Fixing $S^{(p)}$,
\[
\mathbf{E}[ (\tilde\beta - \beta)' S^{(p)}
(\tilde\beta - \beta)] = \langle S^{(p)}, C_{\tilde\beta} \rangle_F
\]
(a Frobenius inner product). A first estimate of paired index differences' variability is thus
\[
2\langle S^{(p)}, C_{\tilde\beta} \rangle_F .
\]

---

## Sampling variability of paired index differences, iii

>- $S^{(p)}$ is based on a designated collection of pairs.  Next goal: a stand-in that's available prior to matching. 
>- We'd like to approximate $(\tilde\beta - \beta)' S^{(p)}
(\tilde\beta - \beta)$ for pairings s.t. $\max_{i\sim j} |(\mathbf{x}_i - \mathbf{x}_j)\beta| \approx 0$. 
>- To emulate $\max_{i\sim j} |(\mathbf{x}_i - \mathbf{x}_j)\beta|=0$, residualize all possible pairs for the true index, leaving remaining differences in place.  For $i\leq n$ and $k\leq p$, let ${x}_{ik}^{\perp\mathbf{x}\beta} =$ residual of $x_{ik}$ regression on $\mathbf{x}\beta$; $\mathbf{x}_i^{\perp\mathbf{x}\beta} = (x^{\perp\mathbf{x}\beta}_{i1}\,x^{\perp\mathbf{x}\beta}_{i2}\, \ldots\, x^{\perp\mathbf{x}\beta}_{ip})$.  By construction, $\max_{i,j}|(\mathbf{x}_i^{\perp\mathbf{x}\beta} - \mathbf{x}_j^{\perp\mathbf{x}\beta})\beta| =0$.  
>- Rather than averaging index difference estimation errors across selected pairs, average across all pairs, but after stripping out differences in $\mathbf{x}\beta$: 
\[
{n \choose 2}^{-1}\mathbf{E}_\beta \sum_{1\leq i < j \leq n} [(\mathbf{x}_i^{\perp\mathbf{x}\beta} - \mathbf{x}_j^{\perp\mathbf{x}\beta}) (\tilde{\beta} - \beta)]^2 =
2 \langle S^{\perp}, C_{\tilde\beta} \rangle_F,
\]
where $S^{\perp} = \frac{1}{2} {n \choose 2}^{-1} \sum_{1\leq i < j \leq n} (\mathbf{x}_i^{\perp\mathbf{x}\beta} - \mathbf{x}_j^{\perp\mathbf{x}\beta})'(\mathbf{x}_i^{\perp\mathbf{x}\beta} - \mathbf{x}_j^{\perp\mathbf{x}\beta}) = \text{Cov}(\mathbf{x}^{\perp\mathbf{x}\beta})$. 
>- To estimate, plug in $\hat{S}^{\perp} = \widehat{\mathrm{Cov}}(\mathbf{x}^{\perp \mathbf{x}\hat{\beta}})$ and $\hat{C}_\hat\beta$. Then $\langle S^{\perp}, C_{\tilde\beta} \rangle_F = O_P(p/n)$ and $\langle \hat{S}^{\perp}, \hat{C}_{\hat\beta} \rangle_F = O_P(p/n)$, while $\langle S^{\perp}, C_{\tilde\beta} \rangle_F - \langle \hat{S}^{\perp}, \hat{C}_{\hat\beta} \rangle_F = o_P(p/n)$. 

*** =pnotes

- Note both modifications -- $\tilde\beta$ not $\hat\beta$, $S^\perp$ not $S$ -- make the mean-square smaller.

---

## Application to Medellin study

-  For caliper, I multiply $\langle \hat{S}^{\perp}, \hat{C}_{\hat\beta} \rangle_F^{1/2}$ by 2.5.

>- Here this works out to 4 logits ($3.9s_p$).
   
>- No neighborhoods excluded.
   <embed src="images/psmodfit0-1.png">
>- (I'll improve treatment of this example presently, but still this is worth noting. Recall that the alternatives excluded at least 12!)

---

# Outline

1. Problem: subject comparability in groupwise comparisons
2. Propensity score caliper matching
3. Diminishing calipers
4. <span class = 'red'>Applications & discussion</span>

--- &twocol

## Example 2: Vascular closure devices vs manual closure following percutaneous coronary intervention


*** =left

>- Once this stent has been threaded up into your heart, the hole in your femoral artery needs to be closed. 
>- VCDs are more comfortable than manual closure -- are they as safe?
>- Gurm et al (2013, _Ann Intern Med_) used data from a 32-hospital collaborative in Michgan to conduct a propensity-matched study. 
>- Large matching problem: $n_1=31$K; $n_0=54$K.  

*** =right

<embed height="400px" width="400px" src="./images/Stent-PCI.jpg">

---


## Calipers to ensure that index differences tend to 0

>- Pairings (denoted "$i \sim j$") should satisfy $\max_{i \sim j} |( \mathbf{x}_i - \mathbf{x}_j)\beta| \downarrow 0$ as $n \uparrow \infty$.
>- Can this be accomplished with a requirement $\max_{i \sim j} |(\mathbf{x}_i - \mathbf{x}_j)\hat\beta| \leq w_n$, some $w_n \downarrow 0$?  (In light of $|(\mathbf{x}_i - \mathbf{x}_j)\beta| \leq |(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \hat\beta)| + |(\mathbf{x}_i - \mathbf{x}_j)\hat\beta|$.)
>- ($w_n = s_p/4$ would mean $w_n \stackrel{P}{\rightarrow} \text{const} > 0$. Again, this won't do; we need $w_n \stackrel{P}{\rightarrow} 0$.)
>- For any $i,j\leq n$, $|(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)| \leq |\mathbf{x}_i - \mathbf{x}_j|_2|\hat\beta - \beta|_2$.
>- Expect $\max_{i,j}|(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)| \approx (\max_{i,j}|\mathbf{x}_i - \mathbf{x}_j|_2)|\hat\beta - \beta|_2$.
>- Even with bounded $x$s, this is $O(p^{1/2})O_P\left[(p/n)^{1/2}\right] = O_P(p/n^{1/2})$ (He & Shao, 2000).
>- We'll need to **assume** $p/n^{1/2} \downarrow 0$ --- not only $n \gg p$, also  $n \gg p^2$! 
>- In that case, any $w_n$ that's $O_P(p/n^{1/2})$ will do the trick.  E.g.,  $w_n \equiv 2.5\langle \hat{S}^{\perp}, \hat{C}_{\hat\beta} \rangle_F^{1/2}$.

---

### Discussion

>- Roughly speaking, the diminishing caliper is 2.5 times the r.m.s. of $\mathbf{x}_i\hat\beta-\mathbf{x}_j\hat\beta$ among pairs $(i,j)$ s.t. $\mathbf{x}_i\beta = \mathbf{x}_j\beta$.
>- Pairs $(i, j)$ s.t. $|(\mathbf{x}_i - \mathbf{x}_j)\hat\beta|$ falls below this caliper are often not distinguishable in $\mathbf{x}\beta$.
>- If $p = o(n/\log(n))$, and side-conditions are enforced, diminishing calipers diminish to 0 as $n\uparrow \infty$.
>- Even with calipers & enforcement of conditions, $\max_{i \sim j}|(\mathbf{x}_i - \mathbf{x}_j)\beta|$ can bigger than the caliper.
>- Still, if $p^{3/2} = o(n/\log(n))$, diminishing caliper matching ensures $\max_{i\sim j} |\mathbf{x}_i\beta-\mathbf{x}_j\beta| \stackrel{P}{\rightarrow} 0$.


<!-- Matches between 1 and 3 ISEs might be considered "marginal" (Austin & Lee, 2009).

### Recommendations: 

>- I'm using sandwich estimates of $\mathrm{Cov} (\hat\beta) $, w/ some special sauce (cf. github.com/benthestatistician/PISE) to limit numerical instability.
>- Our `optmatch` R package (Fredrickson et al, 2016) does optimal pair and full matching (Gu & Rosenbaum, 1993; Hansen & Klopfer, 2006; Stuart & Green, 2008), readily accommodating index calipers.

-->
