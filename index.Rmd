---
title       : Diminishing propensity score calipers
author      : Ben B. Hansen (bbh@umich.edu)
date        : UM Statistics, Nov. 2019
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      #
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
--- 

# Outline

1. <span class = 'red'>Problem: subject comparability in groupwise comparisons</span>
2. Propensity score caliper matching
3. Diminishing calipers
4. Applications & discussion    




--- &twocol    

## Comparability of subjects in groupwise comparisons: canonical scenario with 1-dimensional covariate

*** =left

> - The quasi-experimental setup: $X$, $Y$; $Z \in \{0,1\}$, $Y_c$.
> - <embed height="400px" width="400px" src="./images/pretest-comp-group-design.jpg">

*** =right

> - Observations well outside support of contrasting group should be excluded.
> - Wasteful to exclude members of $\{ i: Z_i=1\}$ because their $x$ is barely outside $\mathrm{range}(\{x_j: Z_j=0\})$.
> -  May distort research question, too.
> - We say $x$ is close to $w$ when we expect $E(Y_{c}|X=x) \approx E(Y_{c}|X=w)$.
> - (...or that $\text{Pr}(Z| X=x) \approx \text{Pr}(Z| X=w)$.)
> - When we lack confidence that $E(Y_{c}|X=x) \approx E(Y_{c}|X=w)$ (or $\text{Pr}(Z| X=x) \approx \text{Pr}(Z| X=w)$), then $x$ and $w$ are not close.   

---

##  Covariate balance -- a separate topic

> - Groupwise comparability is often discussed in terms of whether $\bar{x}_{z=1} \approx \bar{x}_{z=0}$, all or most $X$s.
> - The question presupposes at least rough comparability of subjects: otherwise, mean comparisons can be misleading.
<embed src="./images/balancing-stones.jpg">
> - A topic for another day...

---

## Statistical assumptions for general $X$es

>- Now, multiple $X$es: $\mathbf{X}$, not $X$.
>- Strong ignorability conditions:
>    - No unmeasured confounding: $Y_c \perp Z | \mathbf{X}$.
>    - Overlap: $\mathrm{Pr}(Z=1  | \mathbf{X}) < 1$.      
>- No unmeasured confounding says, there are natural experiments in each "cell" of $\mathbf{X}$.
>- Problem: if multiple ${X}$es, many observations will be all alone in their cells.
>- Rosenbaum & Rubin's (1983) solution is match on an estimate of the propensity score (PS), $\widehat{\mathrm{Pr}}(Z=1| \mathbf{X}=\mathbf{x})$. (Or $g\{\widehat{\mathrm{Pr}}(Z=1| \mathbf{x})\}$, e.g. $g\equiv$ logit.)
>- Second problem: In a cell $\mathbf{x}_0$ s.t. $\mathrm{Pr}(Z=1| \mathbf{X}=\mathbf{x}_0)=1$, there's no data to estimate $\mathrm{Pr}(Y_c \in \cdot |  \mathbf{X}=\mathbf{x}_0)$, even as $n \uparrow \infty$.  (Whether you match on the PS, weight for it, ...)
>- The second problem motivates the (clearly flawed) method of strict overlap, i.e. reducing analytic sample to region of common support on $\widehat{\mathrm{PS}}$: $$\cap_{z=0,1}[\min(\{\widehat{PS}_i:Z_i=z\}), \max(\{\widehat{PS}_i:Z_i=z\})] .$$ 


--- &twocol
##  Example 1: violence & public infrastructure in Medell&iacute;n


*** =left

- Medellin, Colombia (population 2 million)
- As of early 2000s, 60% poverty rate, 20% unemployment, homicide 185 per 100K
- High residential segregation, w/ concentrated poverty in surrounding hills.
- 2004-2006: gondolas connect some but not all to city center.
- Cerda et al (2012, _Am J. Epideomiol._) study effects on neighborhood violence, propensity matching treatment neighborhoods to control.

*** =right

<embed height="625px" width="500px" src="./images/medellin-conc-pov.jpg"> 

*** =pnotes

- In contrast, Detroit's  2007 murder rate was 47/100K, and East St. Louis's was 102 per 100K.

--- &twocolleftwider


## Propensity scores in the Medellin study

*** =left

>- Small matching problem: $n_1=25$, $n_0=23$ (neighborhoods).
>- \(|\bar{x}_{z=1} - \bar{x}_{z=0}|\)s not too large, but PS matching made them smaller.
>- Figure shows PS we matched on - the $\mathbf{x}\hat{\beta}$ from a logistic regression (Cerda et al 2012, _Am J Epi_).
>- Region of strict overlap contains only 6/25 $t$s and 4/23 $c$s!
>- Yet Cerda et al matched all 48 neighborhoods.  (Using "full matching", which permits many-to-one matches.)

*** =right

<embed height="625px" width="400px" src="./images/boxplot-Medellin-logit.jpg">


--- 
# Outline

1. Problem: subject comparability in groupwise comparisons
2. <span class = 'red'>Propensity score caliper matching</span>
3. Diminishing calipers
4. Applications & discussion    





--- &twocolleftwider


## Matching with PS calipers (Rosenbaum & Rubin) 

>- For matching, useful to specify a tolerance or _caliper_ (Althauser & Rubin, 1970). 
>- Absent unmeasured confounding, matches with small PS differences mimic paired random assignment (R.&R. 1983). 
>- In effect, PS caliper (R.&R. 1985) adjudicates "closeness" to region of common support. 

*** =left

>- R. & R. (1985), Rubin & Thomas (2000) recommend $s_p(\widehat{\mathrm{PS}})/4$.
>- (Not $\widehat{\mathrm{PS}}$ the estimated probability, $\widehat{\mathrm{PS}}$ the estimated index function.
>  I.e., logits of estimated probabilities.)
>- The other widely used method (e.g. Austin & Lee 2009, Lunt 2013), besides requiring strict overlap.

*** =right

<embed height="400px" width="200px" src="./images/boxplot-Medellin-logit.jpg">



--- &twocol

## Room for improvement (in caliper = $.25s_p$)

>- $s_p$ does not tend to 0. For large $n$, matches as crude as $.25s_p$ aren't very RCT-like.  (Even if there's no unmeasured confounding.) 
>- In the Medellin study (smallish $n$), $.25s_p(\widehat{\mathrm{PS}})$ caliper excludes 56% of treatment group.
>- At the same time, in that study it's almost tenable ($p=.04$) that all PSes are the same!

*** =left

>- <embed height="400px" width="400px" src="./images/boxplots-Medellin-logit.jpg">

*** =right

>- Moral: in small studies, $.25s_p(\widehat{\mathrm{PS}})$ may be too strict. 
>- In large studies, PS may be estimable w/ much more precision than $.25s_p(\widehat{\mathrm{PS}})$.  At same time, more potential controls. 
>- Moral: in large studies, $.25s_p(\widehat{\mathrm{PS}})$ may be too loose.

--- 

# Outline

1. Problem: subject comparability in groupwise comparisons
2. Propensity score caliper matching
3. <span class = 'red'>Diminishing calipers</span>
4. Applications & discussion    


---

## Sampling variability of paired index differences, i

>- Intuition: Even if we had matched **perfectly** on index $\mathbf{x}\beta$, there would still be matched differences in $\mathbf{x}\hat\beta$.  Let's estimate the size of these differences & use result to define caliper.    
>    - Should imply that as $n\uparrow \infty$, caliper $\downarrow 0$. If also $\hat\beta - \beta \stackrel{P}{\rightarrow} 0$, expect $\max_{i \sim j} |(\mathbf{x}_i - \mathbf{x}_j)\beta|\downarrow 0$, where $i \sim j$ denotes that $i$ is matched to $j$.
>    - Excludes treatment subjects with larger-than-chance separation from controls on $\mathbf{x}\hat\beta$s.    
>- Assuming well-conditioned g.l.m., model dimension $p = o({n}/{\log (n)})$, He & Shao (2000) showed that $|\hat\beta - \beta|_2 = O_P[( {p}/{n})^{1/2}]$.  
>- The error of estimation of paired difference $(\mathbf{x}_i - \mathbf{x}_j)\beta$ is $(\mathbf{x}_i - \mathbf{x}_j)(\hat{\beta} - \beta)$.  Given $\nu$ pairs $(i, j)$, $i < j$ with $i \sim j$, the mean-square of paired errors is
\[
\frac{1}{\nu} \sum_{i \sim j; i < j}
[(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)]^2 =
(\hat\beta - \beta)' \left[\frac{1}{\nu}
                   \sum_{i \lesssim j} (\mathbf{x}_i - \mathbf{x}_j)'(\mathbf{x}_i - \mathbf{x}_j)\right] =: (\hat\beta - \beta)' (2 S^{ p } )
(\hat\beta - \beta).
\]

---

## Sampling variability of paired index differences, ii

>- We wish to estimate the magnitude of $(\hat\beta - \beta)' (2 S^{ p }
) (\hat\beta - \beta)$, with $S^{ p }=$ covariance of $x$s w/in pairs $i \sim j$.
>- Assuming $|S^{ p }|_2 = O_P(1)$, $(\hat\beta - \beta)' S^{ p }
(\hat\beta - \beta) = O_P(p/n)$ (He & Shao, 2000).
>- Define $\tilde\beta = \beta + n^{-1}\sum_{i=1}^{n}\mathrm{IC}({Z}_{i}, \mathbf{x}_{i}; \beta)$. ("IC"=influence curve.) By other results of H. & S.,
\[
(\hat\beta - \beta)' S^{ p }
(\hat\beta - \beta)  =
(\tilde\beta - \beta)' S^{ p }
(\tilde\beta - \beta) + o_P( {p}/{n} ).
\]
>- $(\tilde\beta - \beta)' S^{ p }
(\tilde\beta - \beta)$ is easier to estimate.  For logistic regression & certain other glms, conventional estimates of ${C}_{\tilde\beta} := \text{Cov}(\tilde\beta - \beta)$ satisfy $n|\hat{C}_{\tilde\beta} - {C}_{\tilde\beta}|_2 \stackrel{P}{\rightarrow} 0$, under conditions similar to those H. & S. (2000) used for consistency of $\hat\beta$ (Hansen '19). 
>- Fixing $S^{ p }$,
\[
\mathbf{E}[ (\tilde\beta - \beta)' S^{ p }
(\tilde\beta - \beta)] = \langle S^{ p }, C_{\tilde\beta} \rangle_F
\]
(a Frobenius inner product). A first estimate of paired index differences' variability is thus
\[
2\langle S^{ p }, C_{\tilde\beta} \rangle_F .
\]


<!-- 
>- Considering all ${n \choose 2}$ possible pairs $r$, the expected MS estimation error of paired  differences can be expressed as the Frobenius inner product, $\langle \cdot, \cdot \rangle_F$, of two covariances:   
\[ \mathbf{E}_\beta \left[{n \choose 2}^{-1}\sum_r (\mathbf{d}_r (\hat{\beta} - \beta))^2 \right] = 2\langle S^{(x)}, \mathbf{E}_\beta \{(\hat\beta - \beta) (\hat{\beta} -\beta)' \}\rangle_F . \]
-->

---

## Sampling variability of paired index diffs (ii)

>- $S^{ p }$ is based on a designated collection of pairs.  Next goal: a stand-in that's available prior to matching. 
>- **All possible** pairs include many that are quite different on $\mathbf{x}\beta$.  We wanted to characterize $\mathbf{E} (\mathbf{d}_r(\hat{\beta} - \beta))^2$ among pairs $r$ s.t. $\mathbf{d}_r\beta \approx 0$.  
>- Since there may be few or no such pairs, instead residualize all pairs for the true index, leaving remaining differences in place.  For $i=1,\ldots, p$, define $d_{(i)}^\perp =$ residual of $d_{(i)}$ regression on $\mathbf{d}\beta$; $\mathbf{d}^\perp = (d^\perp_1\,d^\perp_2\, \ldots\, d^\perp_p)$.  (By construction, $\mathbf{d}_r^\perp \beta =0$ for all $r$.)  

>- Rather than the mean-square of index difference estimation errors, ${n \choose 2}^{-1} \mathbf{E} \sum_r (\mathbf{d}_r(\hat{\beta} - \beta))^2$, consider a mean-square in $\tilde\beta$, net of differences in the true index: 
\[
{n \choose 2}^{-1}\mathbf{E}_\beta \sum_r (\mathbf{d}_r^\perp (\tilde{\beta} - \beta))^2 =
2 \langle S^\perp, C_{\tilde\beta} \rangle_F,
\]
where $S^\perp = \frac{1}{2}\mathrm{Cov}(\mathbf{d}^\perp) = \mathrm{Cov}(\mathbf{x}^{\perp \mathbf{x}\beta})$ and $x^{\perp \mathbf{x}\beta} = e(x |\mathbf{1}, \mathbf{x}\beta)$. 
>- Plug in $\hat{S}^\perp = \widehat{\mathrm{Cov}}(\mathbf{x}^{\perp \mathbf{x}\hat{\beta}})$, $\hat{C}_\hat\beta$  from the regression fit to give the "index standard error" (ISE).

*** =pnotes

- IC = influence curve, i.e. linear-in-data approximation to nonlinear estimator. 
- ...
- Note both modifications -- $\tilde\beta$ not $\hat\beta$, $S^\perp$ not $S$ -- make the mean-square smaller.

---

## Application to Medellin study

>- Here 2.5 ISEs works out to 4 logits ($3.9s_p$).
   
>- No neighborhoods excluded.
   <embed src="images/psmodfit0-1.png">
>- (I'll improve treatment of this example presently, but still this is worth noting. Recall that the alternatives excluded at least 12!)

---

# Outline

1. Problem: subject comparability in groupwise comparisons
2. Propensity score caliper matching
3. Diminishing calipers
4. <span class = 'red'>Applications & discussion</span>

--- &twocol

## Example 2: Vascular closure devices vs manual closure following percutaneous coronary intervention


*** =left

>- Once this stent has been threaded up into your heart, the hole in your femoral artery needs to be closed. 
>- VCDs are more comfortable than manual closure -- are they as safe?
>- Gurm et al (2013, _Ann Intern Med_) used data from a 32-hospital collaborative in Michgan to conduct a propensity-matched study. 
>- Large matching problem: $n_1=31$K; $n_0=54$K.  

*** =right

<embed height="400px" width="400px" src="./images/Stent-PCI.jpg">

---


## Calipers to ensure that index differences tend to 0

>- Pairings (denoted "$i \sim j$") should satisfy $\max_{i \sim j} |( \mathbf{x}_i - \mathbf{x}_j)\beta| \downarrow 0$ as $n \uparrow \infty$.
>- Can this be accomplished with a requirement $\max_{i \sim j} |(\mathbf{x}_i - \mathbf{x}_j)\hat\beta| \leq w_n$, some $w_n \downarrow 0$?  (In light of $|(\mathbf{x}_i - \mathbf{x}_j)\beta| \leq |(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \hat\beta)| + |(\mathbf{x}_i - \mathbf{x}_j)\hat\beta|$.)
>- ($w_n = s_p/4$ would mean $w_n \stackrel{P}{\rightarrow} \text{const} > 0$. Again, this won't do; we need $w_n \stackrel{P}{\rightarrow} 0$.)
>- For any $i,j\leq n$, $|(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)| \leq |\mathbf{x}_i - \mathbf{x}_j|_2|\hat\beta - \beta|_2$.
>- Expect $\max_{i,j}|(\mathbf{x}_i - \mathbf{x}_j)(\hat\beta - \beta)| \approx (\max_{i,j}|\mathbf{x}_i - \mathbf{x}_j|_2)|\hat\beta - \beta|_2$.
>- Even with bounded $x$s, this is $O(p^{1/2})O_P\left[(p/n)^{1/2}\right] = O_P(p/n^{1/2})$ (He & Shao, 2000).
>- We'll need to **assume** $p/n^{1/2} \downarrow 0$ --- not only $n \gg p$, also  $n \gg p^2$! 
>- In that case, any $w_n$ that's $O_P(p/n^{1/2})$ will do the trick.  E.g.,  $w_n \equiv 2.5\text{ISE}$.

---

### Discussion

>- Roughly speaking, the ISE is the r.m.s. of $\mathbf{x}_i\hat\beta-\mathbf{x}_j\hat\beta$ among pairs $(i,j)$ s.t. $\mathbf{x}_i\beta = \mathbf{x}_j\beta$. 
>- If $p^2/n \downarrow 0$, then ISE $\stackrel{P}{\rightarrow} 0$, and matching within $k$ ISEs suffices for $\max_{i\sim j} |\mathbf{x}_i\beta-\mathbf{x}_j\beta| \stackrel{P}{\rightarrow} 0$.
>- If $p^2/n \not\rightarrow 0$, no caliper imposed on $\mathbf{x}_i\hat\beta-\mathbf{x}_j\hat\beta$ entails $\max_{i\sim j} |\mathbf{x}_i\beta-\mathbf{x}_j\beta| \stackrel{P}{\rightarrow} 0$.
>- If your matches are farther than a few ISEs, you're matching outside region of overlap. 
>- I like calipers of $k=2.5$ ISEs.


<!-- Matches between 1 and 3 ISEs might be considered "marginal" (Austin & Lee, 2009).-->

### Recommendations: 

>- I'm using sandwich estimates of $\mathrm{Cov} (\hat\beta) $, w/ some special sauce (cf. github.com/benthestatistician/PISE) to limit numerical instability.
>- Our `optmatch` R package (Fredrickson et al, 2016) does optimal pair and full matching (Gu & Rosenbaum, 1993; Hansen & Klopfer, 2006; Stuart & Green, 2008), readily accommodating index calipers.

